{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sp500_company_profiles_file_path=\"resources/sp500_company_profiles.csv\"\n",
    "\n",
    "def get_sp500_list() -> pd.DataFrame:\n",
    "  # URL to the Wikipedia page for S&P 500 constituents\n",
    "  url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "\n",
    "  # Fetch the table of S&P 500 companies\n",
    "  tables = pd.read_html(url)\n",
    "  sp500_table = tables[0]  # The first table contains the list of companies\n",
    "\n",
    "  # Return the symbol and the comany name\n",
    "  return sp500_table[[\"Symbol\", \"Security\"]]\n",
    "\n",
    "sp500_list = get_sp500_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon|Apple|Alphabet|Microsoft|AT&T|Meta|Dell Technologies|Intel|HP Inc|Nvidia|IBM\n"
     ]
    }
   ],
   "source": [
    "def get_us_big_tech() -> str:\n",
    "  url = \"https://en.wikipedia.org/wiki/List_of_largest_technology_companies_by_revenue\"\n",
    "  tables = pd.read_html(url)\n",
    "  big_tech = tables[1]\n",
    "  us_big_tech = big_tech[big_tech[\"Country (origin)\"] == \"US\"]\n",
    "  return \"|\".join(us_big_tech[\"Company\"]).replace(\"Inc.\", \"Inc\")\n",
    "\n",
    "us_big_tech_str_list = get_us_big_tech()\n",
    "print(us_big_tech_str_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Symbol, Security]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def utilise_sp500_company_profiles_csv(sp500_list: pd.DataFrame):\n",
    "  file_path = Path(sp500_company_profiles_file_path)\n",
    "  if(file_path.exists()):\n",
    "    previous_sp500_company_profiles = pd.read_csv(sp500_company_profiles_file_path)\n",
    "    company_symbols = previous_sp500_company_profiles[\"Symbol\"]\n",
    "    new_sp500_list = sp500_list[~sp500_list[\"Symbol\"].isin(company_symbols)]\n",
    "    existing_sp500_company_profile = previous_sp500_company_profiles[company_symbols.isin(sp500_list[\"Symbol\"])]\n",
    "    return {'sp500_list': new_sp500_list, 'sp500_company_profile': existing_sp500_company_profile}\n",
    "  return {'sp500_list': sp500_list, 'sp500_company_profile': None}\n",
    "dist_sp500_list = utilise_sp500_company_profiles_csv(sp500_list)\n",
    "print(dist_sp500_list[\"sp500_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 14)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import finnhub\n",
    "import asyncio\n",
    "\n",
    "from settings import FINNHUB_API_KEY\n",
    "\n",
    "finnhub_client = finnhub.Client(api_key=FINNHUB_API_KEY)\n",
    "\n",
    "def process_sp500_row(row: pd.Series, trail: int=0):\n",
    "  try:\n",
    "    company_profile_json = finnhub_client.company_profile2(symbol=row[\"Symbol\"])\n",
    "    company_profile_json[\"Symbol\"] = row[\"Symbol\"]\n",
    "    print(f\"{row[\"Security\"]}: success\")\n",
    "\n",
    "  except finnhub.FinnhubAPIException as e:\n",
    "    if e.status_code == 429 and trail <= 4:\n",
    "      #Wait for the specified duration before retrying\n",
    "      retry_after = 30+(trail*5)\n",
    "      print(f\"Rate limit exceeded. Retry after {retry_after} seconds\")\n",
    "      time.sleep(retry_after)\n",
    "      company_profile_json = process_sp500_row(row, trail+1)\n",
    "      print(f\"{row['Security']}: {trail}\")\n",
    "    else:\n",
    "      print(f\"Could not retrieve the company profile for {row[\"Security\"]} in {trail} trails\")\n",
    "      company_profile_json = None\n",
    "  return company_profile_json\n",
    "\n",
    "# async def async_process_sp500_row(\n",
    "#     row: pd.Series,\n",
    "#     loop: asyncio.AbstractEventLoop,\n",
    "#     executor: ThreadPoolExecutor):\n",
    "#   result = await loop.run_in_executor(executor, process_sp500_row, row)\n",
    "#   return result\n",
    "\n",
    "\n",
    "# async def async_process_sp500_company_profiles(sp500_list: pd.DataFrame):\n",
    "#   loop = asyncio.get_running_loop()\n",
    "#   with ThreadPoolExecutor() as executor:\n",
    "#     sp500_list = sp500_list.head(5)\n",
    "#     tasks = [async_process_sp500_row(row, loop, executor) for index, row in sp500_list.iterrows()]\n",
    "#     sp500_company_profiles = await asyncio.gather(*tasks)\n",
    "#   return sp500_company_profiles\n",
    "\n",
    "async def async_process_sp500_row(\n",
    "    row: pd.Series,\n",
    "    semaphore: asyncio.Semaphore):\n",
    "  async with semaphore:\n",
    "    return process_sp500_row(row)\n",
    "\n",
    "async def async_process_sp500_company_profiles(sp500_list: pd.DataFrame, concurrency_limit: int = 3):\n",
    "  semaphore = asyncio.Semaphore(concurrency_limit)\n",
    "  tasks = [async_process_sp500_row(row, semaphore) for _, row in sp500_list.iterrows()]\n",
    "  sp500_company_profiles = await asyncio.gather(*tasks)\n",
    "  return sp500_company_profiles\n",
    "\n",
    "current_sp500_list = dist_sp500_list[\"sp500_list\"]\n",
    "sp500_company_profiles_json = await async_process_sp500_company_profiles(current_sp500_list)\n",
    "fetched_sp500_company_profiles = pd.DataFrame(sp500_company_profiles_json)\n",
    "sp500_company_profiles = pd.concat([dist_sp500_list[\"sp500_company_profile\"], fetched_sp500_company_profiles], ignore_index=True)\n",
    "# print(sp500_company_profiles)\n",
    "\n",
    "sp500_company_profiles.to_csv(sp500_company_profiles_file_path, na_rep=\"N/A\", index=False)\n",
    "nasdaq_sp500 = sp500_company_profiles[sp500_company_profiles[\"name\"].str.contains(us_big_tech_str_list, case=False, na=False)]\n",
    "nasdaq_sp500.shape\n",
    "# sp500_company_profiles = pd.DataFrame(sp500_company_profiles_json)\n",
    "# print(sp500_company_profiles)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#   # sp500_company_profiles = asyncio.run(async_process_sp500_company_profiles(sp500_list)) # Uncomment if not in Jupyter notebooks. When running in Juptyer notebooks, it already has a running event loop thus no need to run a new event loop. This means we only need to wait on the async task\n",
    "#   sp500_company_profiles = await async_process_sp500_company_profiles(sp500_list) # Comment if in Jupyter notebooks\n",
    "#   sp500_company_profiles = pd.DataFrame(sp500_company_profiles_json)\n",
    "#   print(sp500_company_profiles.dtypes)\n",
    "#   print(sp500_company_profiles[\"ticker\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = nasdaq_sp500[\"ticker\"]\n",
    "import websocket\n",
    "\n",
    "def on_message(ws, message):\n",
    "    print(message)\n",
    "\n",
    "def on_error(ws, error):\n",
    "    print(error)\n",
    "\n",
    "def on_close(ws):\n",
    "    print(\"### closed ###\")\n",
    "\n",
    "def on_open(ws):\n",
    "    ticker_list_5 = ticker_list.head(100)\n",
    "    for ticker in ticker_list_5:\n",
    "        # ticker=\"GOOGL\"\n",
    "        ws.send(f'{{\"type\":\"subscribe\", \"symbol\":\"{ticker}\"}}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    websocket.enableTrace(True)\n",
    "    ws = websocket.WebSocketApp(\"wss://ws.finnhub.io?token=cugj791r01qr6jncvar0cugj791r01qr6jncvarg\",\n",
    "                              on_message = on_message,\n",
    "                              on_error = on_error,\n",
    "                              on_close = on_close)\n",
    "    ws.on_open = on_open\n",
    "    ws.run_forever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 30, 35]\n"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame({\n",
    "#     \"id\": [1, 2, 3],\n",
    "#     \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "#     \"age\": [25, 30, 35]\n",
    "# })\n",
    "x = {\n",
    "    \"id\": [1, 2, 3],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"age\": [25, 30, 35]\n",
    "}\n",
    "id, age, name = x.values()\n",
    "\n",
    "print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/malemichael/miniconda3/envs/finnhubDPL/lib/python3.13/site-packages\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "print(os.path.realpath(os.path.dirname(sys.argv[0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finnhubDPL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
